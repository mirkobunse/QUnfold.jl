<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API reference · QUnfold.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">QUnfold.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API reference</a><ul class="internal"><li><a class="tocitem" href="#Common-interface"><span>Common interface</span></a></li><li><a class="tocitem" href="#Quantification-/-unfolding-methods"><span>Quantification / unfolding methods</span></a></li><li><a class="tocitem" href="#Feature-transformations"><span>Feature transformations</span></a></li></ul></li><li><a class="tocitem" href="../experiments/">Experiments</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mirkobunse/QUnfold.jl/blob/master/docs/src/api-reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-reference"><a class="docs-heading-anchor" href="#API-reference">API reference</a><a id="API-reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-reference" title="Permalink"></a></h1><p>Below, you find a listing of all public methods of this package. Any other method you might find in the source code is not intended for direct usage.</p><h2 id="Common-interface"><a class="docs-heading-anchor" href="#Common-interface">Common interface</a><a id="Common-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Common-interface" title="Permalink"></a></h2><p>TODO with an exemplary link to <a href="#QUnfold.fit"><code>fit</code></a>.</p><article class="docstring"><header><a class="docstring-binding" id="QUnfold.fit" href="#QUnfold.fit"><code>QUnfold.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">fit(m, X, y) -&gt; FittedMethod</code></pre><p>Return a copy of the QUnfold method <code>m</code> that is fitted to the data set <code>(X, y)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L75-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="QUnfold.predict" href="#QUnfold.predict"><code>QUnfold.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predict(m, X) -&gt; Vector{Float64}</code></pre><p>Predict the class prevalences in the data set <code>X</code> with the fitted method <code>m</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L90-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="QUnfold.predict_with_background" href="#QUnfold.predict_with_background"><code>QUnfold.predict_with_background</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predict_with_background(m, X, X_b, α=1) -&gt; Vector{Float64}</code></pre><p>Predict the class prevalences in the observed data set <code>X</code> with the fitted method <code>m</code>, taking into account a background measurement <code>X_b</code> that is scaled by <code>α</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L98-L104">source</a></section></article><h2 id="Quantification-/-unfolding-methods"><a class="docs-heading-anchor" href="#Quantification-/-unfolding-methods">Quantification / unfolding methods</a><a id="Quantification-/-unfolding-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Quantification-/-unfolding-methods" title="Permalink"></a></h2><h3 id="CC"><a class="docs-heading-anchor" href="#CC">CC</a><a id="CC-1"></a><a class="docs-heading-anchor-permalink" href="#CC" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.CC" href="#QUnfold.CC"><code>QUnfold.CC</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">CC(classifier; kwargs...)</code></pre><p>The Classify &amp; Count method, which uses crisp classifier predictions without any adjustment. This weak baseline method is proposed by Forman, 2008: <em>Quantifying counts and costs via classification</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L190-L198">source</a></section></article><h3 id="ACC"><a class="docs-heading-anchor" href="#ACC">ACC</a><a id="ACC-1"></a><a class="docs-heading-anchor-permalink" href="#ACC" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.ACC" href="#QUnfold.ACC"><code>QUnfold.ACC</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ACC(classifier; kwargs...)</code></pre><p>The Adjusted Classify &amp; Count method, which solves a least squares objective with crisp classifier predictions.</p><p>A regularization strength <code>τ &gt; 0</code> yields the o-ACC method for ordinal quantification, which is proposed by Bunse et al., 2022: <em>Ordinal Quantification through Regularization</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 0.0</code> is the regularization strength for o-ACC.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul><p><strong>Strategies</strong></p><p>For binary classification, ACC is proposed by Forman, 2008: <em>Quantifying counts and costs via classification</em>. In the multi-class setting, multiple extensions are available.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:pinv</code> computes a pseudo-inverse akin to a minimum-norm constraint, as discussed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:inv</code> computes the true inverse (if existent) of the transfer matrix <code>M</code>, as proposed by Vucetic &amp; Obradovic, 2001: <em>Classification on data with biased class distribution</em>.</li><li><code>:ovr</code> solves multiple binary one-versus-rest adjustments, as proposed by Forman (2008).</li><li><code>:none</code> yields the <code>CC</code> method without any adjustment.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L130-L156">source</a></section></article><h3 id="PCC"><a class="docs-heading-anchor" href="#PCC">PCC</a><a id="PCC-1"></a><a class="docs-heading-anchor-permalink" href="#PCC" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.PCC" href="#QUnfold.PCC"><code>QUnfold.PCC</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">PCC(classifier; kwargs...)</code></pre><p>The Probabilistic Classify &amp; Countmethod, which uses predictions of posterior probabilities without any adjustment. This method is proposed by Bella et al., 2010: <em>Quantification via Probability Estimators</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L202-L210">source</a></section></article><h3 id="PACC"><a class="docs-heading-anchor" href="#PACC">PACC</a><a id="PACC-1"></a><a class="docs-heading-anchor-permalink" href="#PACC" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.PACC" href="#QUnfold.PACC"><code>QUnfold.PACC</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">PACC(classifier; kwargs...)</code></pre><p>The Probabilistic Adjusted Classify &amp; Count method, which solves a least squares objective with predictions of posterior probabilities.</p><p>A regularization strength <code>τ &gt; 0</code> yields the o-PACC method for ordinal quantification, which is proposed by Bunse et al., 2022: <em>Ordinal Quantification through Regularization</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 0.0</code> is the regularization strength for o-PACC.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul><p><strong>Strategies</strong></p><p>For binary classification, PACC is proposed by Bella et al., 2010: <em>Quantification via Probability Estimators</em>. In the multi-class setting, multiple extensions are available.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:pinv</code> computes a pseudo-inverse akin to a minimum-norm constraint, as discussed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:inv</code> computes the true inverse (if existent) of the transfer matrix <code>M</code>, as proposed by Vucetic &amp; Obradovic, 2001: <em>Classification on data with biased class distribution</em>.</li><li><code>:ovr</code> solves multiple binary one-versus-rest adjustments, as proposed by Forman (2008).</li><li><code>:none</code> yields the <code>CC</code> method without any adjustment.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L160-L186">source</a></section></article><h3 id="RUN"><a class="docs-heading-anchor" href="#RUN">RUN</a><a id="RUN-1"></a><a class="docs-heading-anchor-permalink" href="#RUN" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.RUN" href="#QUnfold.RUN"><code>QUnfold.RUN</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">RUN(transformer; kwargs...)</code></pre><p>The Regularized Unfolding method by Blobel, 1985: <em>Unfolding methods in high-energy physics experiments</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 1e-6</code> is the regularization strength for ordinal quantification.</li><li><code>n_df = -1</code> (only used if <code>strategy==:original</code>) is the effective number of degrees of freedom, required to be <code>0 &lt; n_df &lt;= C</code> where <code>C</code> is the number of classes.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li></ul><p><strong>Strategies</strong></p><p>Blobel&#39;s loss function, feature transformation, and regularization can be optimized with multiple strategies.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:original</code> is the original, unconstrained Newton optimization proposed by Blobel (1985).</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li><li><code>:unconstrained</code> (our method) is similar to <code>:original</code>, but uses a more generic solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L255-L277">source</a></section></article><h3 id="SVD"><a class="docs-heading-anchor" href="#SVD">SVD</a><a id="SVD-1"></a><a class="docs-heading-anchor-permalink" href="#SVD" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.SVD" href="#QUnfold.SVD"><code>QUnfold.SVD</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">SVD(transformer; kwargs...)</code></pre><p>The The Singular Value Decomposition-based unfolding method by Hoecker &amp; Kartvelishvili, 1996: <em>SVD approach to data unfolding</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 1e-6</code> is the regularization strength for ordinal quantification.</li><li><code>n_df = -1</code> (only used if <code>strategy==:original</code>) is the effective rank, required to be <code>0 &lt; n_df &lt; C</code> where <code>C</code> is the number of classes.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li></ul><p><strong>Strategies</strong></p><p>Hoecker &amp; Kartvelishvili&#39;s loss function, feature transformation, and regularization can be optimized with multiple strategies.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:original</code> is the original, analytic solution proposed by Hoecker &amp; Kartvelishvili (1996).</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li><li><code>:unconstrained</code> (our method) is similar to <code>:original</code>, but uses a more generic solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L281-L303">source</a></section></article><h3 id="HDx"><a class="docs-heading-anchor" href="#HDx">HDx</a><a id="HDx-1"></a><a class="docs-heading-anchor-permalink" href="#HDx" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.HDx" href="#QUnfold.HDx"><code>QUnfold.HDx</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HDx(n_bins; kwargs...)</code></pre><p>The Hellinger Distance-based method on feature histograms by González-Castro et al., 2013: <em>Class distribution estimation based on the Hellinger distance</em>.</p><p>The parameter <code>n_bins</code> specifies the number of bins <em>per feature</em>. A regularization strength <code>τ &gt; 0</code> yields the o-HDx method for ordinal quantification, which is proposed by Bunse et al., 2022: <em>Machine learning for acquiring knowledge in astro-particle physics</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 0.0</code> is the regularization strength for o-HDx.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li></ul><p><strong>Strategies</strong></p><p>González-Castro et al.&#39;s loss function and feature transformation can be optimized with multiple strategies.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L325-L346">source</a></section></article><h3 id="HDy"><a class="docs-heading-anchor" href="#HDy">HDy</a><a id="HDy-1"></a><a class="docs-heading-anchor-permalink" href="#HDy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.HDy" href="#QUnfold.HDy"><code>QUnfold.HDy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HDy(classifier, n_bins; kwargs...)</code></pre><p>The Hellinger Distance-based method on prediction histograms by González-Castro et al., 2013: <em>Class distribution estimation based on the Hellinger distance</em>.</p><p>The parameter <code>n_bins</code> specifies the number of bins <em>per class</em>. A regularization strength <code>τ &gt; 0</code> yields the o-HDx method for ordinal quantification, which is proposed by Bunse et al., 2022: <em>Machine learning for acquiring knowledge in astro-particle physics</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>strategy = :softmax</code> is the solution strategy (see below).</li><li><code>τ = 0.0</code> is the regularization strength for o-HDx.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul><p><strong>Strategies</strong></p><p>González-Castro et al.&#39;s loss function and feature transformation can be optimized with multiple strategies.</p><ul><li><code>:softmax</code> (default; our method) improves <code>:softmax_full_reg</code> by setting one latent parameter to zero instead of introducing a technical regularization term.</li><li><code>:constrained</code> constrains the optimization to proper probability densities, as proposed by Hopkins &amp; King, 2010: <em>A method of automated nonparametric content analysis for social science</em>.</li><li><code>:softmax_full_reg</code> (our method) introduces a soft-max layer, which makes contraints obsolete. This strategy employs a technical regularization term, as proposed by Bunse, 2022: <em>On Multi-Class Extensions of Adjusted Classify and Count</em>.</li><li><code>:softmax_reg</code> (our method) is a variant of <code>:softmax</code>, which sets one latent parameter to zero in addition to introducing a technical regularization term.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L355-L377">source</a></section></article><h3 id="IBU"><a class="docs-heading-anchor" href="#IBU">IBU</a><a id="IBU-1"></a><a class="docs-heading-anchor-permalink" href="#IBU" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.IBU" href="#QUnfold.IBU"><code>QUnfold.IBU</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">IBU(transformer, n_bins; kwargs...)</code></pre><p>The Iterative Bayesian Unfolding method by D&#39;Agostini, 1995: <em>A multidimensional unfolding method based on Bayes&#39; theorem</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>o = 0</code> is the order of the polynomial for ordinal quantification.</li><li><code>λ = 0.0</code> is the impact of the polynomial for ordinal quantification.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L403-L413">source</a></section></article><h3 id="SLD"><a class="docs-heading-anchor" href="#SLD">SLD</a><a id="SLD-1"></a><a class="docs-heading-anchor-permalink" href="#SLD" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="QUnfold.SLD" href="#QUnfold.SLD"><code>QUnfold.SLD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SLD(classifier; kwargs...)</code></pre><p>The Saerens-Latinne-Decaestecker method, a.k.a. EMQ or Expectation Maximization-based Quantification by Saerens et al., 2002: <em>Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure</em>.</p><p>A polynomial order <code>o &gt; 0</code> and regularization impact <code>λ &gt; 0</code> yield the o-SLD method for ordinal quantification, which is proposed by Bunse et al., 2022: <em>Machine learning for acquiring knowledge in astro-particle physics</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>o = 0</code> is the order of the polynomial for o-SLD.</li><li><code>λ = 0.0</code> is the impact of the polynomial for o-SLD.</li><li><code>a = Float64[]</code> are the acceptance factors for unfolding analyses.</li><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/QUnfold.jl#L428-L441">source</a></section></article><h2 id="Feature-transformations"><a class="docs-heading-anchor" href="#Feature-transformations">Feature transformations</a><a id="Feature-transformations-1"></a><a class="docs-heading-anchor-permalink" href="#Feature-transformations" title="Permalink"></a></h2><p>The unfolding methods <a href="#QUnfold.RUN"><code>RUN</code></a>, <a href="#QUnfold.SVD"><code>SVD</code></a>, and <a href="#QUnfold.IBU"><code>IBU</code></a> have the flexibility of choosing between different feature transformations.</p><article class="docstring"><header><a class="docstring-binding" id="QUnfold.ClassTransformer" href="#QUnfold.ClassTransformer"><code>QUnfold.ClassTransformer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ClassTransformer(classifier; kwargs...)</code></pre><p>This transformer yields the classification-based feature transformation used in <code>ACC</code>, <code>PACC</code>, <code>CC</code>, <code>PCC</code>, and <code>SLD</code>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>is_probabilistic = false</code> whether or not to use posterior predictions.</li><li><code>fit_classifier = true</code> whether or not to fit the given <code>classifier</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/transformers.jl#L47-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="QUnfold.TreeTransformer" href="#QUnfold.TreeTransformer"><code>QUnfold.TreeTransformer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TreeTransformer(tree; kwargs...)</code></pre><p>This transformer yields a tree-induced partitioning, as proposed by Börner et al., 2017: <em>Measurement/simulation mismatches and multivariate data discretization in the machine learning era</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>fit_frac = 1/5</code> is the fraction of data used for training the tree if <code>fit_tree==true</code>.</li><li><code>fit_tree = true</code> whether or not to fit the given <code>tree</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/transformers.jl#L172-L181">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="QUnfold.HistogramTransformer" href="#QUnfold.HistogramTransformer"><code>QUnfold.HistogramTransformer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HistogramTransformer(n_bins; kwargs...)</code></pre><p>This transformer yields the histogram-based feature transformation used in <code>HDx</code> and <code>HDy</code>. The parameter <code>n_bins</code> specifies the number of bins <em>per input feature</em>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>preprocessor = nothing</code> can be another <code>AbstractTransformer</code> that is called before this transformer.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mirkobunse/QUnfold.jl/blob/00dc36cdb01460101d4b2f07d3f188d256ee0dae/src/transformers.jl#L112-L120">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../experiments/">Experiments »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 25 January 2023 13:08">Wednesday 25 January 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
